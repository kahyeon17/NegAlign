# P_align ì‚´ë¦¬ê¸° ìž‘ì „ ðŸš€

## ðŸ“Š í˜„ìž¬ ìƒí™© ë¶„ì„

### ë¬¸ì œì  ì§„ë‹¨

```
ID (ImageNet):     P_align = -0.1193
OOD (iNaturalist): P_align = -0.1286
Gap:               0.0093 â† TOO SMALL! âŒ

OOD (NINCO):       P_align = -0.1235
Gap:               0.0042 â† EVEN SMALLER! âŒ

OOD (Texture):     P_align = -0.1416
Gap:               0.0223 â† SLIGHTLY BETTER, BUT WRONG DIRECTION âš ï¸
```

**í•µì‹¬ ë¬¸ì œ:**
1. **P_alignì´ ëª¨ë‘ ìŒìˆ˜** â†’ Backgroundê°€ predicted classë³´ë‹¤ generic negativesì™€ ë” ìœ ì‚¬
2. **ID-OOD gapì´ ë„ˆë¬´ ìž‘ìŒ** (0.004~0.022) â†’ íŒë³„ë ¥ ì—†ìŒ
3. **Textureì—ì„œ gapì´ ì—­ë°©í–¥** â†’ OODê°€ ë” ìŒìˆ˜ â†’ ìž˜ëª»ëœ ì‹ í˜¸

---

## ðŸŽ¯ P_align ê°œì„  ì „ëžµ (ìš°ì„ ìˆœìœ„ ìˆœ)

### Strategy 1: **ë¶€í˜¸ ë°˜ì „** (ê°€ìž¥ ê°„ë‹¨, ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥) â­â­â­

**ê°€ì„¤:** í˜„ìž¬ P_alignì˜ ë¶€í˜¸ê°€ ìž˜ëª»ë¨

```python
# í˜„ìž¬ (v1_original)
P_align = S_bg_pos - S_bg_neg  # OODê°€ ë” ìŒìˆ˜ â†’ ìž˜ëª»ëœ ë°©í–¥

# ì œì•ˆ (v2_sign_flip)
P_align = S_bg_neg - S_bg_pos  # = -(S_bg_pos - S_bg_neg)
```

**ê¸°ëŒ€ íš¨ê³¼:**
- Texture: gap = -0.0223 â†’ +0.0223 (ì˜¬ë°”ë¥¸ ë°©í–¥)
- iNaturalist: gap = -0.0093 â†’ +0.0093
- Lambda > 0ì¼ ë•Œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥

**í…ŒìŠ¤íŠ¸ ë°©ë²•:**
```bash
python test_p_align_variants.py \
  --ood_dataset texture \
  --n_samples 500 \
  --variants v2_sign_flip \
  --device cuda:0
```

---

### Strategy 2: **Ratio-based Scoring** (Gap ì¦í­) â­â­â­

**ê°€ì„¤:** ì°¨ì´(difference)ë³´ë‹¤ ë¹„ìœ¨(ratio)ì´ ë” í° gapì„ ë§Œë“¦

```python
# v3_ratio
P_align = S_bg_pos / (S_bg_neg + 1e-8)

# Example:
# ID:  S_bg_pos=-0.10, S_bg_neg=-0.12 â†’ ratio = 0.833
# OOD: S_bg_pos=-0.11, S_bg_neg=-0.14 â†’ ratio = 0.786
# Gap: 0.047 (ì›ëž˜ 0.01ì—ì„œ 4ë°° ì¦í­!)
```

**ìž¥ì :**
- Differenceì˜ ìž‘ì€ gapì„ ratioë¡œ ì¦í­
- ìŒìˆ˜ ê°’ì´ì–´ë„ ë¹„ìœ¨ë¡œ íŒë³„ ê°€ëŠ¥

**í…ŒìŠ¤íŠ¸:**
```bash
python test_p_align_variants.py \
  --ood_dataset texture \
  --variants v3_ratio v4_log_ratio
```

---

### Strategy 3: **Negative-Only Scoring** (Simplification) â­â­

**ê°€ì„¤:** S_bg_posê°€ noiseì¼ ìˆ˜ ìžˆìŒ. BackgroundëŠ” negativesë¡œë§Œ íŒë³„

```python
# v5_neg_only
P_align = -S_bg_neg

# ì§ê´€: OOD backgroundëŠ” generic negativesì™€ ë” ìœ ì‚¬ (S_bg_negê°€ ëœ ìŒìˆ˜)
# Example:
# ID:  S_bg_neg=-0.12 â†’ P_align=0.12
# OOD: S_bg_neg=-0.14 â†’ P_align=0.14
# Gap: 0.02 (í‹€ë¦¼! ì—­ë°©í–¥) âŒ

# ìˆ˜ì •: S_bg_neg ìžì²´ë¥¼ ì‚¬ìš©
P_align = S_bg_neg  # ë” ìŒìˆ˜ì¼ìˆ˜ë¡ OOD-like
```

**í…ŒìŠ¤íŠ¸:**
```bash
python test_p_align_variants.py \
  --variants v5_neg_only v6_pos_only
```

---

### Strategy 4: **Foreground-Background Contrast** (ê·¼ë³¸ì  ìž¬ì„¤ê³„) â­â­â­â­

**ê°€ì„¤:** Backgroundë§Œ ë³´ì§€ ë§ê³ , FGì™€ BGì˜ **ëŒ€ë¹„**ë¥¼ ë´ì•¼ í•¨

```python
# í˜„ìž¬: Background embeddingë§Œ ì¶”ì¶œ
bg_embedding = extract_bg_embedding(image, bg_mask)
P_align = S_bg_pos - S_bg_neg

# ì œì•ˆ: Foreground embeddingë„ ì¶”ì¶œ
fg_embedding = extract_fg_embedding(image, fg_mask)

# Foregroundì™€ Backgroundì˜ class-alignment ì°¨ì´
S_fg_pos = fg_embedding @ text_feature_c_hat
S_bg_pos = bg_embedding @ text_feature_c_hat

P_align = S_fg_pos - S_bg_pos  # FG-BG separation
```

**ì§ê´€:**
- **ID:** Foregroundê°€ classì™€ ê°•í•˜ê²Œ align â†’ S_fg_pos >> S_bg_pos â†’ **ë†’ì€ P_align**
- **OOD:** Background biasë¡œ FG-BG êµ¬ë¶„ ì•½í•¨ â†’ S_fg_pos â‰ˆ S_bg_pos â†’ **ë‚®ì€ P_align**

**ìž¥ì :**
- Background noiseì— ëœ ë¯¼ê°
- FG-BG separationì´ background biasì˜ ì§ì ‘ì  ì§€í‘œ

**êµ¬í˜„ í•„ìš”:**
```python
def extract_fg_embedding(clip_model, image_tensor, fg_mask):
    """Extract foreground embedding using CAM mask."""
    # Get patch embeddings (before projection)
    patch_tokens = clip_model.visual.forward_patches(image_tensor)

    # Apply foreground mask
    fg_embedding = masked_pooling(patch_tokens, fg_mask, mode='mean')

    # Project to CLIP space
    fg_embedding = fg_embedding @ clip_model.visual.proj
    fg_embedding = fg_embedding / fg_embedding.norm()

    return fg_embedding
```

---

### Strategy 5: **Multi-Scale CAM** (Background ì¶”ì¶œ ê°œì„ ) â­â­

**ê°€ì„¤:** Single-block CAMì´ ë¶€ì •í™•í•¨ â†’ Multi-scale averaging í•„ìš”

```python
# í˜„ìž¬: Last block only
cam = compute_cam(image, block_idx=-1)

# ì œì•ˆ: Multiple blocks
cams = []
for block_idx in [-1, -2, -3]:  # Last 3 blocks
    cam_i = compute_cam(image, block_idx)
    cams.append(cam_i)

# Weighted average (later blocks have more weight)
weights = [0.5, 0.3, 0.2]
multi_scale_cam = sum(w * c for w, c in zip(weights, cams))
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ë” ì •í™•í•œ foreground/background ë¶„ë¦¬
- Edge leakage ê°ì†Œ

---

### Strategy 6: **Adaptive Threshold** (Per-image calibration) â­â­

**ê°€ì„¤:** Fixed percentile (80th)ì´ ëª¨ë“  ì´ë¯¸ì§€ì— ì í•©í•˜ì§€ ì•ŠìŒ

```python
# í˜„ìž¬: Fixed 80th percentile
threshold = np.percentile(cam, 80)

# ì œì•ˆ: Adaptive threshold (Otsu's method)
from skimage.filters import threshold_otsu
threshold = threshold_otsu(cam)

# ë˜ëŠ”: Entropy-based
def entropy_threshold(cam):
    """Find threshold that maximizes entropy difference."""
    best_threshold = 0
    best_entropy_diff = 0

    for percentile in range(50, 95, 5):
        t = np.percentile(cam, percentile)
        fg = cam >= t
        bg = cam < t

        # Compute entropy difference
        fg_entropy = -np.sum(cam[fg] * np.log(cam[fg] + 1e-8))
        bg_entropy = -np.sum(cam[bg] * np.log(cam[bg] + 1e-8))
        entropy_diff = abs(fg_entropy - bg_entropy)

        if entropy_diff > best_entropy_diff:
            best_entropy_diff = entropy_diff
            best_threshold = t

    return best_threshold
```

---

### Strategy 7: **Learned Lambda Weighting** (Data-driven) â­â­â­â­

**ê°€ì„¤:** Fixed Î»ëŠ” suboptimal â†’ Learn from data

```python
# Option A: Per-dataset optimal lambda (validation set)
validation_results = {}
for lambda_val in [0, 0.5, 1.0, 2.0, 5.0, 10.0]:
    auroc = evaluate_with_lambda(lambda_val, val_id, val_ood)
    validation_results[lambda_val] = auroc

best_lambda = max(validation_results, key=validation_results.get)

# Option B: Adaptive lambda based on P_align confidence
def adaptive_lambda(p_align, cam_valid):
    """Higher lambda when P_align is more confident."""
    if not cam_valid:
        return 0.0

    # Use P_align magnitude as confidence
    confidence = abs(p_align)

    # Scale lambda inversely with confidence
    # Low confidence â†’ low weight
    lambda_val = min(10.0, confidence * 50.0)

    return lambda_val

# Option C: Learn linear combination
# S_final = Î± * S_star + Î² * P_align
# Learn Î±, Î² from validation set using logistic regression
```

---

## ðŸ§ª ì‹¤í—˜ í”„ë¡œí† ì½œ

### Phase 1: Quick Variant Testing (1-2ì‹œê°„)

```bash
# Test all variants on small sample
python test_p_align_variants.py \
  --ood_dataset texture \
  --n_samples 500 \
  --device cuda:0

# Expected output: variant ranking with improvement scores
```

**Decision Point:**
- If **any variant shows >1% improvement** â†’ Proceed to Phase 2
- If **no improvement** â†’ Move to Strategy 4 (FG-BG contrast)

---

### Phase 2: Best Variant Full Test (2-3ì‹œê°„)

```bash
# Test best variant on full dataset
python test_imagenet_validation.py \
  --p_align_variant v2_sign_flip \  # Or best from Phase 1
  --samples_per_class 5 \
  --max_ood_samples 5000
```

**Success Criteria:**
- AUROC improvement > 0.5% on at least 1 OOD dataset
- Best Î» > 0 (P_align actually used)

---

### Phase 3: FG-BG Contrast (if Phase 1/2 fail) (1ì¼)

**Implement new method:**

```python
# In clip_negalign_v2.py, add:

def _compute_p_align_fg_bg(self, image_tensor, predicted_class_idx):
    """Foreground-Background contrast version."""
    text_feature_c_hat = self.pos_features[predicted_class_idx]

    # Get CAM
    cam = self.cam_generator.compute_cam(image_tensor, text_feature_c_hat)

    # Get FG and BG masks
    fg_mask, bg_mask = cam_to_masks(cam, self.cam_fg_percentile, self.cam_dilate_px)

    # Extract both embeddings
    fg_embedding = extract_fg_embedding(self.clip_model, image_tensor, fg_mask)
    bg_embedding = extract_bg_embedding(self.clip_model, image_tensor, bg_mask)

    # Compute class alignment for each
    s_fg_pos = float((fg_embedding @ text_feature_c_hat).item())
    s_bg_pos = float((bg_embedding @ text_feature_c_hat).item())

    # P_align = FG-BG separation
    p_align = s_fg_pos - s_bg_pos

    return p_align, True
```

**Expected Improvement:**
- Gap should be **5-10x larger** (í˜„ìž¬ 0.01 â†’ 0.05~0.1)
- More interpretable (FG-BG separation is what we want)

---

### Phase 4: Multi-Scale + Adaptive (ìµœì¢… ë‹¨ê³„) (2ì¼)

Combine best strategies:
1. FG-BG contrast
2. Multi-scale CAM
3. Adaptive threshold
4. Learned lambda

---

## ðŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### Conservative Estimate (Strategy 1-3ë§Œ)

```
í˜„ìž¬ (Best Î»=0.0):
  iNaturalist: 0.9977
  NINCO:       0.8041
  Texture:     0.9382

Strategy 2 (v2_sign_flip) ì„±ê³µ ì‹œ:
  iNaturalist: 0.9977 â†’ 0.9980 (+0.03%)
  NINCO:       0.8041 â†’ 0.8090 (+0.49%) âœ…
  Texture:     0.9382 â†’ 0.9420 (+0.38%) âœ…
```

### Optimistic Estimate (Strategy 4 FG-BG contrast)

```
FG-BG contrast ì„±ê³µ ì‹œ:
  iNaturalist: 0.9977 â†’ 0.9985 (+0.08%)
  NINCO:       0.8041 â†’ 0.8200 (+1.59%) ðŸ”¥
  Texture:     0.9382 â†’ 0.9500 (+1.18%) ðŸ”¥
```

### Best Case (All strategies combined)

```
Multi-scale + FG-BG + Learned Î»:
  iNaturalist: 0.9977 â†’ 0.9990 (+0.13%)
  NINCO:       0.8041 â†’ 0.8300 (+2.59%) ðŸš€
  Texture:     0.9382 â†’ 0.9550 (+1.68%) ðŸš€
```

---

## âœ… ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Immediate Actions (ì˜¤ëŠ˜)

- [ ] **Run test_p_align_variants.py** on Texture dataset
  ```bash
  cd /home/kahyeon/research/NegAlign
  python test_p_align_variants.py \
    --ood_dataset texture \
    --n_samples 500 \
    --device cuda:0
  ```

- [ ] **Analyze results** and pick best variant
- [ ] **If successful:** Test on full dataset
- [ ] **If failed:** Start implementing FG-BG contrast

### Short-term (ì´ë²ˆ ì£¼)

- [ ] Implement `extract_fg_embedding()` function
- [ ] Add `v9_fg_bg_contrast` variant
- [ ] Test FG-BG contrast on all 3 OOD datasets
- [ ] Compare with baseline

### Medium-term (ë‹¤ìŒ ì£¼)

- [ ] Implement multi-scale CAM
- [ ] Add adaptive threshold
- [ ] Validation-based lambda tuning
- [ ] Write ablation study section for paper

---

## ðŸŽ“ ë…¼ë¬¸ ìž‘ì„± ì „ëžµ (P_align ì„±ê³µ ì‹œ)

### Main Story

**Title:** "Foreground-Background Contrast for Background-Bias-Aware OOD Detection"

**Key Contributions:**
1. Role-aware negative vocabulary (N_obj vs N_bg)
2. **FG-BG contrast metric** for background bias detection
3. Multi-scale CAM for accurate region separation
4. Adaptive weighting for dataset-specific calibration

**Positioning:**
- First to use FG-BG contrast for OOD detection
- Interpretable metric (FG-BG separation)
- Strong empirical results on background-biased OOD

---

## ðŸŽ“ ë…¼ë¬¸ ìž‘ì„± ì „ëžµ (P_align ì‹¤íŒ¨ ì‹œ)

### Honest Analysis Paper

**Title:** "What Makes Background Calibration Effective? An Empirical Study"

**Key Contributions:**
1. Comprehensive analysis of P_align variants (8 variants tested)
2. **Negative result:** Simple background alignment insufficient
3. **Finding:** FG-BG contrast necessary but not sufficient
4. Dataset-specific recommendations (when to use what)

**Value:**
- Important negative results for community
- Deep analysis of failure modes
- Future research directions

---

## ðŸ“ž ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ ì‹¤í–‰:**
```bash
cd /home/kahyeon/research/NegAlign
chmod +x test_p_align_variants.py
python test_p_align_variants.py --ood_dataset texture --n_samples 500
```

**ê²°ê³¼ í™•ì¸ í›„:**
1. **ê°œì„  ìžˆìŒ (>1%):** Full dataset test ì§„í–‰
2. **ê°œì„  ë¯¸ë¯¸ (<1%):** FG-BG contrast êµ¬í˜„ ì‹œìž‘
3. **ì„±ëŠ¥ ì•…í™”:** ì›ì¸ ë¶„ì„ ë° ëŒ€ì•ˆ ê²€í† 

**ì½”ë“œ íŒŒì¼:**
- `clip_negalign_v2.py`: 8ê°€ì§€ P_align variants êµ¬í˜„ ì™„ë£Œ âœ…
- `test_p_align_variants.py`: Variant testing script ì™„ë£Œ âœ…
- ë‹¤ìŒ: `cam_bg_v2.py` (FG-BG contrastìš© fg_embedding ì¶”ì¶œ)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- Phase 1 (variant test): 1-2ì‹œê°„
- Phase 2 (full test): 2-3ì‹œê°„
- Phase 3 (FG-BG): 1ì¼
- Phase 4 (final): 2ì¼

**Total:** 3-4ì¼ì´ë©´ P_align ì™„ì „ í•´ê²° ê°€ëŠ¥! ðŸš€
